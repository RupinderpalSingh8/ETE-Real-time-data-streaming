# Real-Time Kafka Streaming Pipeline

## ğŸ“Œ Overview
In today's digital-first era, data is the driving force behind innovation. This project showcases an **end-to-end Kafka streaming pipeline** that transforms raw API data into actionable insights in real time. The pipeline is built using **Apache Kafka, Apache Spark, Apache Cassandra, Apache Airflow, and Docker** to ensure **scalability, resilience, and real-time processing**.

## ğŸš€ Why Build This Pipeline?
A robust data streaming pipeline isn't just a technological achievementâ€”itâ€™s a **game-changer** for businesses looking to stay ahead. Hereâ€™s why real-time data streaming is essential:

âœ… **Immediate Insights:** Enables quick, data-driven decision-making.
âœ… **Scalability:** Manages high data volumes with ease using distributed systems.
âœ… **Resilience:** Ensures fault tolerance and handles unexpected data spikes gracefully.
âœ… **Innovation:** Opens doors to AI/ML integration and next-gen analytics.

---

## âš™ï¸ Technologies Used
- **Docker** â€“ Containerized environment for seamless deployment
- **Apache Airflow** â€“ Workflow automation and scheduling
- **Apache Kafka** â€“ High-speed data streaming
- **Apache Spark** â€“ Real-time data transformation
- **Apache Cassandra** â€“ Scalable, fault-tolerant data storage
- **Python** â€“ Custom API data retrieval & processing scripts

---

## ğŸ—ï¸ Architecture & Workflow

### 1ï¸âƒ£ **Setting Up with Docker & Apache Airflow**
- The entire environment is **containerized** using Docker.
- **Apache Airflow DAG** automates scheduled API data fetching.
- A **custom Python operator** ensures **data validation and transformation**.

### 2ï¸âƒ£ **Streaming Data with Apache Kafka**
- Kafka acts as a **real-time message broker** for API data.
- Kafka setup includes **Zookeeper and a schema registry** for scalability.

### 3ï¸âƒ£ **Processing Data in Real Time with Apache Spark**
- Spark reads data **directly from Kafka** and processes it on the fly.
- Structured streaming enables **efficient transformations**.

### 4ï¸âƒ£ **Storing Insights in Apache Cassandra**
- Processed data is **stored in Apache Cassandra** for querying and analysis.
- Cassandraâ€™s **distributed nature** ensures high availability.

---

## ğŸ› ï¸ Installation & Setup

### 1ï¸âƒ£ **Clone the Repository**
```bash
 git clone https://github.com/RupinderpalSingh8/ETE-Real-time-data-streaming.git
 cd kafka-streaming-pipeline
```

### 2ï¸âƒ£ **Run the Services using Docker Compose**
```bash
 docker-compose up -d
```
This starts all necessary services (**Kafka, Zookeeper, Spark, Cassandra, and Airflow**).

### 3ï¸âƒ£ **Check Running Containers**
```bash
 docker ps
```

### 4ï¸âƒ£ **Start the Python Scrip**
- Go to terminal
  ```bash
    python spark-stream.py
  ```

### 5ï¸âƒ£ **Access Airflow UI (for DAG Scheduling)**
- Open your browser and go to: `http://localhost:8080`
- Default credentials:
  - **Username:** `airflow`
  - **Password:** `airflow`
 
### 6ï¸âƒ£ **Start the Airflow DAG**
- Enable the DAG to **fetch API data** and push it to Kafka.

---

## ğŸ¤ Contributing
Contributions are welcome! Feel free to submit **issues**, **pull requests**, or **feature requests**.

---

---

## ğŸ“¬ Contact
For questions or collaboration, reach out at: **rupinderpalsinghraze@gmail.com**
